{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk4sent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsklREwXuK6+TE0dnpDLyr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxkleiner/maXbox/blob/master/nltk4sent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE9-2uRfoemj",
        "colab_type": "code",
        "outputId": "eaa81b4d-9875-4761-cae8-bf2adbfbacf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "import string\n",
        "\n",
        "from nltk.corpus import movie_reviews as mr\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5083YQoort_",
        "colab_type": "text"
      },
      "source": [
        "But the no. of unique words in a corpus can be very huge. We could restrict our model to extract features that are the most salient. But to do so we need to know what is the top N most frequent words in the corpus. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmHyiSbioxl5",
        "colab_type": "code",
        "outputId": "f906836a-617f-486a-bc93-83776792cd8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# nltk.download('stopwords')\n",
        "# nltk.download('movie_reviews')\n",
        "stop = stopwords.words('english') + list(string.punctuation)\n",
        "# This will extract key-value pairs, \n",
        "# where the keys are words and values are its count in the corpus.\n",
        "vocabulary = FreqDist(w.lower() for w in mr.words() if w.lower() not in stop)\n",
        "print(vocabulary.most_common(10))\n",
        "#print(list(vocabulary.keys())[:10])\n",
        "# the least uses words\n",
        "print(vocabulary.most_common()[-10:])\n",
        "\n",
        "#word_features = FreqDist(chain(*[i for i,j in documents]))\n",
        "word_features = list(vocabulary.keys())[:10]\n",
        "print(word_features)\n",
        "\n",
        "# vocabulary[1:50]\n",
        "#vocabulary\n",
        "\n",
        "documents = [([w for w in mr.words(i) if w.lower() not in stop], # Words in document.\n",
        "               i.split('/')[0]) # Tag.\n",
        "                for i in mr.fileids()]\n",
        "print(documents[0][:10]) # First document"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('film', 9517), ('one', 5852), ('movie', 5771), ('like', 3690), ('even', 2565), ('good', 2411), ('time', 2411), ('story', 2169), ('would', 2109), ('much', 2049)]\n",
            "[('paneled', 1), ('vainly', 1), ('snoots', 1), ('obstructions', 1), ('obscuring', 1), ('tangerine', 1), ('timbre', 1), ('powaqqatsi', 1), ('keyboardist', 1), ('capitalized', 1)]\n",
            "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get']\n",
            "(['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mind', 'fuck', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'film', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'half', 'way', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', '20', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'might', 'pretty', 'decent', 'teen', 'mind', 'fuck', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'character', 'unraveling', 'overall', 'film', 'stick', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'joblo', 'coming', 'nightmare', 'elm', 'street', '3', '7', '10', 'blair', 'witch', '2', '7', '10', 'crow', '9', '10', 'crow', 'salvation', '4', '10', 'lost', 'highway', '10', '10', 'memento', '10', '10', 'others', '9', '10', 'stir', 'echoes', '8', '10'], 'neg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EVJCUKV1WTT",
        "colab_type": "text"
      },
      "source": [
        "Classify the sentiment from NLTK to Scikit\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXWtb4WP1lGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#X = mr.words\n",
        "#y = mr.tag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeIdxog_6u6F",
        "colab_type": "text"
      },
      "source": [
        "An NLTK's CategorizedPlaintextCorpusReader object isn't a dtype for pandas.\n",
        "\n",
        "That being said, you can convert the movie reviews into list of tuples and then populate a dataframe as such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvSNA4WP6yk3",
        "colab_type": "code",
        "outputId": "f5d30f8d-f698-4968-a15e-0d202f360568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import movie_reviews as mr\n",
        "\n",
        "reviews = []\n",
        "for fileid in mr.fileids():\n",
        "    tag, filename = fileid.split('/')\n",
        "    reviews.append((filename, tag, mr.raw(fileid)))\n",
        "\n",
        "df = pd.DataFrame(reviews, columns=['filename', 'tag', 'text'])\n",
        "\n",
        "df.head(7)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>tag</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv000_29416.txt</td>\n",
              "      <td>neg</td>\n",
              "      <td>plot : two teen couples go to a church party ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cv001_19502.txt</td>\n",
              "      <td>neg</td>\n",
              "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cv002_17424.txt</td>\n",
              "      <td>neg</td>\n",
              "      <td>it is movies like these that make a jaded movi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cv003_12683.txt</td>\n",
              "      <td>neg</td>\n",
              "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cv004_12641.txt</td>\n",
              "      <td>neg</td>\n",
              "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cv005_29357.txt</td>\n",
              "      <td>neg</td>\n",
              "      <td>capsule : in 2176 on the planet mars police ta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>cv006_17022.txt</td>\n",
              "      <td>neg</td>\n",
              "      <td>so ask yourself what \" 8mm \" ( \" eight millime...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          filename  tag                                               text\n",
              "0  cv000_29416.txt  neg  plot : two teen couples go to a church party ,...\n",
              "1  cv001_19502.txt  neg  the happy bastard's quick movie review \\ndamn ...\n",
              "2  cv002_17424.txt  neg  it is movies like these that make a jaded movi...\n",
              "3  cv003_12683.txt  neg   \" quest for camelot \" is warner bros . ' firs...\n",
              "4  cv004_12641.txt  neg  synopsis : a mentally unstable man undergoing ...\n",
              "5  cv005_29357.txt  neg  capsule : in 2176 on the planet mars police ta...\n",
              "6  cv006_17022.txt  neg  so ask yourself what \" 8mm \" ( \" eight millime..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVs4hKKy7tH_",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to classify\n",
        "The purpose is to use this data for sentiment analysis. while converting the data using pandas, glorious dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bVJAex68By_",
        "colab_type": "code",
        "outputId": "f8a8a9d6-21fe-4551-cb23-943ba1ade405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "stoplist = stopwords.words('english') + list(string.punctuation)\n",
        "def preprocess(atext):\n",
        "    return [ps.stem(word) for word in nltk.word_tokenize(atext) \\\n",
        "                 if word.lower() not in stoplist and not word.isdigit()]\n",
        "\n",
        "# it doesnt apply the preprocess to the text vocab, shape remains the same?!\n",
        "# df['text'].apply(preprocess) \n",
        "\n",
        "X = df['text']\n",
        "y = df['tag']\n",
        "X= X.map(lambda x:' '.join([ps.stem(word) for word in x.split(' ') \\\n",
        "                   # if not word in stoplist and not word.isdigit()]))\n",
        "                     if word.lower() not in stoplist and not word.isdigit()]))\n",
        "print('neg:',sum(df['tag']=='neg'), 'pos:',sum(df['tag']=='pos'))\n",
        "print('single word count ',sum(df.text.str.count('film')))\n",
        "print('preproc: ','\\n',X[0:5])\n",
        "\n",
        "# df= pd.read_csv('../input/movie-review/movie_review.csv') no need cause of df\n",
        "\n",
        "vect = CountVectorizer(ngram_range=(1, 2), min_df=3)\n",
        "#vect = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', min_df=4, max_df=0.8) \n",
        "#, preprocessor=preprocess)\n",
        "\n",
        "\n",
        "Xv = vect.fit_transform(X)\n",
        "print('shape',Xv.shape)\n",
        "\n",
        "# Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "         train_test_split(Xv, y, test_size=0.3, random_state=42, stratify=None, shuffle=True)\n",
        "\n",
        "\n",
        "# model = BernoulliNB(alpha=1.0)\n",
        "model = svm.LinearSVC(random_state=0, C=0.01) #, gamma='scale')\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "p_train = model.predict(X_train)\n",
        "p_test = model.predict(X_test)\n",
        "\n",
        "acc_train = accuracy_score(y_train, p_train)\n",
        "acc_test = accuracy_score(y_test, p_test)\n",
        "\n",
        "print(f'Train ACC: {acc_train}, Test ACC: {acc_test}')\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_train, p_train, labels=['neg', 'pos']),'\\n')\n",
        "print(confusion_matrix(y_test, p_test, labels=['neg', 'pos']),'\\n')\n",
        "\n",
        "print(df.info())\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "neg: 1000 pos: 1000\n",
            "single word count  11607\n",
            "preproc:  \n",
            " 0    plot two teen coupl go church parti drink driv...\n",
            "1    happi bastard' quick movi review \\ndamn y2k bu...\n",
            "2    movi like make jade movi viewer thank invent t...\n",
            "3     quest camelot warner bro first feature-length...\n",
            "4    synopsi mental unstabl man undergo psychothera...\n",
            "Name: text, dtype: object\n",
            "shape (2000, 46184)\n",
            "Train ACC: 1.0, Test ACC: 0.835\n",
            "[[698   0]\n",
            " [  0 702]] \n",
            "\n",
            "[[255  47]\n",
            " [ 52 246]] \n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   filename  2000 non-null   object\n",
            " 1   tag       2000 non-null   object\n",
            " 2   text      2000 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 47.0+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZd8U2ikl32H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas.api.types import is_categorical_dtype\n",
        "#import shap\n",
        "def cat_to_code(df_orig):\n",
        "    \"\"\"Convert dataframe with categoricals to numerical array\"\"\"\n",
        "    df_mod = df_orig.copy()\n",
        "    for col in df_orig.columns:\n",
        "        if is_categorical_dtype(df_orig[col]):\n",
        "            df_mod[col] = df_orig[col].cat.codes\n",
        "    X = df_mod.values\n",
        "    return X\n",
        "\n",
        "#x_codes = cat_to_code(X)\n",
        "\n",
        "#exp = shap.TreeExplainer(model=model, data=x_codes)\n",
        "#print(exp.shap_values(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtHwyH4JPmMI",
        "colab_type": "text"
      },
      "source": [
        "In the end we check the distribution of negative and positive contribution words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRcCpuiFP0v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.probability import FreqDist, ConditionalFreqDist\n",
        "word_fd = FreqDist()\n",
        "label_word_fd = ConditionalFreqDist()\n",
        "stops = stopwords.words('english') + list(string.punctuation)\n",
        "\n",
        "#polar = [w.lower() for w in mr.words() if w.lower() not in stops]\n",
        "\n",
        "def best_word_feats(words):\n",
        "    return dict([(word, True) for word in words if word not in stops])\n",
        "\n",
        "all_words = [word.lower() for word in mr.words()]\n",
        "# print first 10 words\n",
        "print (all_words[:10])\n",
        "\n",
        "all_words_clean = []\n",
        "for word in all_words:\n",
        "    if word not in stops:\n",
        "        all_words_clean.append(word)\n",
        " \n",
        "print (all_words_clean[:10])\n",
        "print ('distinctive words: ',len(set(all_words_clean)))\n",
        "\n",
        "all_words_freq = FreqDist(all_words_clean)\n",
        "print (all_words_freq)\n",
        "\n",
        "# for word in best_word_feats(mr.words(categories=['neg'])):\n",
        "for word in mr.words(categories=['neg']):\n",
        "    #word_fd.inc(word.lower())\n",
        "    all_words_freq[word.lower()] +=1 \n",
        "    label_word_fd['neg'][word.lower()] +=1\n",
        "print(all_words_freq.most_common()[:20])\n",
        "print(all_words_freq.most_common()[-20:])\n",
        " \n",
        "for word in mr.words(categories=['pos']):\n",
        "    all_words_freq[word.lower()] += 1\n",
        "    label_word_fd['pos'][word.lower()] += 1  \n",
        "print(label_word_fd['pos'].most_common()[:20])\n",
        "print(label_word_fd['pos'].most_common()[-20:]) \n",
        "\n",
        "neg_word_count = label_word_fd['neg'].N()\n",
        "pos_word_count = label_word_fd['pos'].N()\n",
        "\n",
        "# print 10 most frequently occurring words\n",
        "print (all_words_freq.most_common(20))\n",
        "\n",
        "print(neg_word_count, pos_word_count)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def feat_importances(coef, names):\n",
        "    imp = coef\n",
        "    imp,names = zip(*sorted(zip(imp,names)))\n",
        "    plt.barh(range(len(names)), imp, align='center')\n",
        "    plt.yticks(range(len(names)), names)\n",
        "    plt.show()\n",
        "\n",
        "#feat_importances(model.coef_, features_names)\n",
        "#importances = model.feature_importances_\n",
        "print(vect.get_feature_names()[0:15])\n",
        "# pd.Series(abs(model.coef_[0]), index=vect.get_feature_names).nlargest(10).plot(kind='barh')\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}