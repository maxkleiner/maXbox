{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of simple_image_classification_with_any_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxkleiner/maXbox/blob/master/Copy_of_simple_image_classification_with_any_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21WmyQrYYFGo",
        "colab_type": "text"
      },
      "source": [
        "## Simple architecture trained with any dataset\n",
        "You can learn more about these datasets at:\n",
        "* CIFAR-10 and CIFAR-100: https://www.cs.toronto.edu/~kriz/cifar.html .\n",
        "* MNIST: http://yann.lecun.com/exdb/mnist/ .\n",
        "* FASHION MNIST: https://github.com/zalandoresearch/fashion-mnist .\n",
        "\n",
        "This example is part of the [K-CAI Neural API](https://github.com/joaopauloschuler/k-neural-api)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br3ws9jlWuaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Global Settings\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.layers\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.datasets\n",
        "from tensorflow.keras import regularizers\n",
        "dataset=tensorflow.keras.datasets.fashion_mnist #@param [\"tensorflow.keras.datasets.cifar10\", \"tensorflow.keras.datasets.cifar100\", \"tensorflow.keras.datasets.mnist\", \"tensorflow.keras.datasets.fashion_mnist\"] {type:\"raw\"} \n",
        "batch_size=64 # @param [32, 64, 128, 256, 512] {type:\"raw\"} \n",
        "epochs=32 # @param [32, 64, 128, 256, 512] {type:\"raw\"}\n",
        "l2_decay = 0.000001 #@param {type:\"number\"}\n",
        "initial_learning_rate = 0.01 #@param {type:\"number\"}\n",
        "verbose=True #@param {type:\"boolean\"}\n",
        "bipolar_input=True #@param {type:\"boolean\"}\n",
        "\n",
        "if dataset is keras.datasets.cifar10 or dataset is keras.datasets.cifar100:\n",
        "  input_shape = (32, 32, 3)\n",
        "else:\n",
        "  input_shape = (28, 28, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxIOe64-Cf8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa3d60fd-4a49-4bdc-866d-d26817a5fb56"
      },
      "source": [
        "import os\n",
        "if not os.path.isdir('k'):\n",
        "  !git clone https://github.com/joaopauloschuler/k-neural-api.git k\n",
        "else:\n",
        "  !cd k && git pull\n",
        "!cd k && pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'k'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 356 (delta 38), reused 38 (delta 18), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (356/356), 224.47 KiB | 631.00 KiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "Processing /content/k\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from cai==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from cai==0.1.0) (1.0.5)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from cai==0.1.0) (0.16.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.2.30 in /usr/local/lib/python3.6/dist-packages (from cai==0.1.0) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0numpy in /usr/local/lib/python3.6/dist-packages (from cai==0.1.0) (0.22.2.post1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (2.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (1.30.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (3.2.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (1.6.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (1.12.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2.0->cai==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->cai==0.1.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->cai==0.1.0) (2018.9)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->cai==0.1.0) (2.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->cai==0.1.0) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->cai==0.1.0) (3.2.2)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->cai==0.1.0) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->cai==0.1.0) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0numpy->cai==0.1.0) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow>=2.2.0->cai==0.1.0) (49.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (1.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.15.0->cai==0.1.0) (4.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.0) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->cai==0.1.0) (0.4.8)\n",
            "Building wheels for collected packages: cai\n",
            "  Building wheel for cai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cai: filename=cai-0.1.0-cp36-none-any.whl size=21013 sha256=6fd9a9d2042ea91fdb19e1d472f73763455d420c4450364830160266aa3ca0b0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ykytkaa3/wheels/de/4a/3e/5eaf9592f16dae0db5670d85206898522fd9b7a7238081da99\n",
            "Successfully built cai\n",
            "Installing collected packages: cai\n",
            "Successfully installed cai-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM06Ebn9qmvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "03f15639-0da3-4f77-9ce3-af6263312cf9"
      },
      "source": [
        "import cai.densenet\n",
        "import cai.util\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "cai.util.create_folder_if_required(save_dir)\n",
        "base_model_name = os.path.join(save_dir, 'simple_model_on_any_dataset')\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(64, (5, 5), padding='valid',\n",
        "                 input_shape=input_shape, kernel_regularizer=regularizers.l2(l2_decay)) )\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(keras.layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_decay)) )\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_decay)) )\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(32, kernel_regularizer=regularizers.l2(l2_decay)) )\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.Dense(32, kernel_regularizer=regularizers.l2(l2_decay)) )\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "if (dataset==keras.datasets.cifar100):\n",
        "  model.add(keras.layers.Dense(100))\n",
        "else:\n",
        "  model.add(keras.layers.Dense(10))\n",
        "model.add(keras.layers.Activation('softmax'))\n",
        "\n",
        "if (verbose):\n",
        "    model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 6, 6, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 6, 6, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                73760     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 150,666\n",
            "Trainable params: 150,666\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAVLC73e2KF9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6aabab4d-aec5-4c3a-acc6-bcfd65ab802d"
      },
      "source": [
        "# Learning rate schedule.\n",
        "def lrscheduler(epoch):\n",
        "  return initial_learning_rate * (0.99**epoch)\n",
        "\n",
        "fit_result,  model_name,  csv_name = cai.datasets.train_model_on_dataset(model, dataset, base_model_name, \n",
        "  plrscheduler=lrscheduler, batch_size=batch_size, epochs=epochs, lab=False, verbose=verbose, bipolar=bipolar_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "train shape (60000, 28, 28)\n",
            "test shape (10000, 28, 28)\n",
            "Loading RGB.\n",
            "Channel  0  min: -2.0  max: 1.984\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/cai/datasets.py:650: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.79270, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 21s - loss: 0.8354 - accuracy: 0.6847 - val_loss: 0.5361 - val_accuracy: 0.7927 - lr: 0.0100\n",
            "Epoch 2/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.79270 to 0.80620, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 22s - loss: 0.5995 - accuracy: 0.7747 - val_loss: 0.5102 - val_accuracy: 0.8062 - lr: 0.0099\n",
            "Epoch 3/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.80620 to 0.84140, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 23s - loss: 0.5388 - accuracy: 0.7960 - val_loss: 0.4192 - val_accuracy: 0.8414 - lr: 0.0098\n",
            "Epoch 4/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.84140 to 0.84510, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 22s - loss: 0.4932 - accuracy: 0.8139 - val_loss: 0.4229 - val_accuracy: 0.8451 - lr: 0.0097\n",
            "Epoch 5/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.84510 to 0.85440, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 22s - loss: 0.4693 - accuracy: 0.8227 - val_loss: 0.3962 - val_accuracy: 0.8544 - lr: 0.0096\n",
            "Epoch 6/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.85440\n",
            "937/937 - 22s - loss: 0.4521 - accuracy: 0.8313 - val_loss: 0.3984 - val_accuracy: 0.8534 - lr: 0.0095\n",
            "Epoch 7/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.85440 to 0.85790, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 22s - loss: 0.4370 - accuracy: 0.8367 - val_loss: 0.3785 - val_accuracy: 0.8579 - lr: 0.0094\n",
            "Epoch 8/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.85790 to 0.86690, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 22s - loss: 0.4283 - accuracy: 0.8398 - val_loss: 0.3639 - val_accuracy: 0.8669 - lr: 0.0093\n",
            "Epoch 9/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.86690\n",
            "937/937 - 22s - loss: 0.4132 - accuracy: 0.8444 - val_loss: 0.4424 - val_accuracy: 0.8430 - lr: 0.0092\n",
            "Epoch 10/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.86690 to 0.86720, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 22s - loss: 0.4059 - accuracy: 0.8448 - val_loss: 0.3576 - val_accuracy: 0.8672 - lr: 0.0091\n",
            "Epoch 11/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.86720\n",
            "937/937 - 23s - loss: 0.3994 - accuracy: 0.8492 - val_loss: 0.3759 - val_accuracy: 0.8634 - lr: 0.0090\n",
            "Epoch 12/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.86720\n",
            "937/937 - 22s - loss: 0.3875 - accuracy: 0.8547 - val_loss: 0.3681 - val_accuracy: 0.8661 - lr: 0.0090\n",
            "Epoch 13/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.86720 to 0.87090, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 22s - loss: 0.3867 - accuracy: 0.8555 - val_loss: 0.3693 - val_accuracy: 0.8709 - lr: 0.0089\n",
            "Epoch 14/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.87090 to 0.87460, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 23s - loss: 0.3814 - accuracy: 0.8567 - val_loss: 0.3509 - val_accuracy: 0.8746 - lr: 0.0088\n",
            "Epoch 15/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.87460\n",
            "937/937 - 23s - loss: 0.3766 - accuracy: 0.8604 - val_loss: 0.4075 - val_accuracy: 0.8538 - lr: 0.0087\n",
            "Epoch 16/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.87460\n",
            "937/937 - 23s - loss: 0.3706 - accuracy: 0.8606 - val_loss: 0.3782 - val_accuracy: 0.8654 - lr: 0.0086\n",
            "Epoch 17/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.87460 to 0.87520, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 23s - loss: 0.3694 - accuracy: 0.8609 - val_loss: 0.3422 - val_accuracy: 0.8752 - lr: 0.0085\n",
            "Epoch 18/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.87520\n",
            "937/937 - 22s - loss: 0.3631 - accuracy: 0.8636 - val_loss: 0.3744 - val_accuracy: 0.8696 - lr: 0.0084\n",
            "Epoch 19/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.87520\n",
            "937/937 - 22s - loss: 0.3582 - accuracy: 0.8656 - val_loss: 0.4120 - val_accuracy: 0.8557 - lr: 0.0083\n",
            "Epoch 20/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.87520 to 0.87540, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 23s - loss: 0.3575 - accuracy: 0.8661 - val_loss: 0.3508 - val_accuracy: 0.8754 - lr: 0.0083\n",
            "Epoch 21/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.87540 to 0.88050, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 23s - loss: 0.3530 - accuracy: 0.8682 - val_loss: 0.3362 - val_accuracy: 0.8805 - lr: 0.0082\n",
            "Epoch 22/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88050\n",
            "937/937 - 23s - loss: 0.3475 - accuracy: 0.8704 - val_loss: 0.3697 - val_accuracy: 0.8724 - lr: 0.0081\n",
            "Epoch 23/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88050\n",
            "937/937 - 23s - loss: 0.3462 - accuracy: 0.8694 - val_loss: 0.3531 - val_accuracy: 0.8764 - lr: 0.0080\n",
            "Epoch 24/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.88050 to 0.88310, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 23s - loss: 0.3421 - accuracy: 0.8709 - val_loss: 0.3460 - val_accuracy: 0.8831 - lr: 0.0079\n",
            "Epoch 25/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88310\n",
            "937/937 - 23s - loss: 0.3353 - accuracy: 0.8744 - val_loss: 0.3531 - val_accuracy: 0.8794 - lr: 0.0079\n",
            "Epoch 26/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88310\n",
            "937/937 - 23s - loss: 0.3387 - accuracy: 0.8723 - val_loss: 0.3529 - val_accuracy: 0.8741 - lr: 0.0078\n",
            "Epoch 27/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88310\n",
            "937/937 - 23s - loss: 0.3341 - accuracy: 0.8756 - val_loss: 0.3711 - val_accuracy: 0.8718 - lr: 0.0077\n",
            "Epoch 28/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00028: val_accuracy improved from 0.88310 to 0.88330, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 23s - loss: 0.3308 - accuracy: 0.8741 - val_loss: 0.3357 - val_accuracy: 0.8833 - lr: 0.0076\n",
            "Epoch 29/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88330\n",
            "937/937 - 23s - loss: 0.3273 - accuracy: 0.8779 - val_loss: 0.3486 - val_accuracy: 0.8813 - lr: 0.0075\n",
            "Epoch 30/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.88330 to 0.89030, saving model to /content/saved_models/simple_model_on_any_dataset.h5\n",
            "937/937 - 23s - loss: 0.3266 - accuracy: 0.8786 - val_loss: 0.3125 - val_accuracy: 0.8903 - lr: 0.0075\n",
            "Epoch 31/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.89030\n",
            "937/937 - 23s - loss: 0.3313 - accuracy: 0.8765 - val_loss: 0.3562 - val_accuracy: 0.8737 - lr: 0.0074\n",
            "Epoch 32/32\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89030\n",
            "937/937 - 23s - loss: 0.3272 - accuracy: 0.8778 - val_loss: 0.3404 - val_accuracy: 0.8805 - lr: 0.0073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DOhvTFCyZmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38-wYOFH0IeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEZW_z8B7gsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d436dfc5-dbe9-4b2d-a219-f1a1e754c3d0"
      },
      "source": [
        "files.download(csv_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3d0438ba-4a1c-4619-acaf-1d51f47decd5\", \"simple_model_on_any_dataset.csv\", 2979)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lclbSixe8iiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "90ef7835-80c6-4f2b-a67a-d5d881e1e7c8"
      },
      "source": [
        "files.download(model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_af27cfe4-5650-454e-9556-775d3bd7dd57\", \"simple_model_on_any_dataset.h5\", 1266944)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}